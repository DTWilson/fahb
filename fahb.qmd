---
title: "Feasibility Assessment with Hierarchical Bayes"
subtitle: "(fahb)"
format: 
  revealjs:
    theme: serif
execute:
  echo: false
---

```{r}
#| label: setup
library(reshape2)
library(ggplot2)
library(brms)
library(posterior)
library(qgam)

cols <- c("#ffba49", "#20a39e", "#ef5b5b", "#23001e", "#a4a9ad")
```



We will use GUSTO as our motivating example:

- Recruiting 320 participants from 20 sites over 3 years.
- Assessment of recruitment at 6 months:
    * Stop if $\leq 9$ patients recruited and $\leq 2$ sites are open;
    * Go if $\geq 30$ patients recruited and $\geq 5$ sites open.
    
Will we have enough information at 6 months to make good progression decisions?

# Modelling recruitment

##

If we expect different sites to recruit at different rates $\lambda_1, \lambda_2, \ldots , \lambda_m$, we can use a hierarchical model for the number recruited at site $i$ at 6 months:

$$
n_i \sim Poisson(\lambda_i) \\
\lambda_i \sim Gamma(\alpha, \beta).
$$

Analogous to cluster RCTs, it is the number of sites (clusters) which really drives precision. 

##

```{r, dev.args = list(bg = 'transparent')}
df <- data.frame(r = seq(0,30,0.1))
df$p <- dgamma(df$r, 2, 0.3)

ggplot(df, aes(r, p)) + geom_line() +
  ylab("Probability") + xlab("Recruitment rate") +
  theme_minimal() + 
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
```

##

We can model the number of sites set up at 6 months using a Poisson distribution with rate $\tau$:

$$
m_p \sim Poisson(\tau).
$$
Our full recruitment model then has three parameters, $\alpha, \beta$, and $\tau$, and we specify some hypothetical prior distributions for each of these.

# Analysing recruitment

##

At 6 months, we can estimate posterior distributions for:

- The recruitment rates for the open sites, $\lambda_1, \ldots , \lambda_{m_p}$;
- The recruitment rates for the un-open sites, $\lambda_{m_P + 1}, \ldots , \lambda_m$;
- The rate at which sites will continue to open, $\tau$.

We can put all these together to obtain a predictive distribution of the total number of participants recruited at 3 years.

##

For example, suppose we land on the lower of the progression criteria thresholds, with 2 sites open at 6 months, having recruited 3 and 6 participants each.

In theory, this should lead to a fairly confident prediction that the final sample size will be signficantly below the target.

```{r}
ln_pars <- function(m, s) {
  # Log-normal parameters which will give a distribution with mean m and sd s
  mu <- (4*log(m) - log(s^2 + m^2))/2
  sig <- sqrt(2*(log(m) - mu))
  
  return(c(mu, sig))
  
  # check
  #exp(mu + sig^2/2); sqrt( (exp(sig^2) - 1)*exp(2*mu + sig^2) )
}

sim_rec_setup <- function(m, int_t, mu, sig, setup_r) {
  # True recruitment rates per site per year
  lambdas <- exp(rnorm(m, mu, sig))
  
  # Setup times - setup_r is expected number of sites in a year
  setup_ts <- cumsum(rexp(m, setup_r))
  
  pilot_sites <- which(setup_ts <= int_t)
  m_p <- length(pilot_sites)
  #print(m_p)
  
  # Total recruitment rate by interim
  lambda_p <- sum(lambdas[pilot_sites]*(int_t - setup_ts[pilot_sites])/int_t)
  
  # Total recruited at interim
  n_p <- rpois(1, lambda_p)
  
  # Site distribution
  if(m_p > 0){
    ns <- rmultinom(1, n_p, (lambdas[pilot_sites]*(int_t - setup_ts[pilot_sites])/int_t)/lambda_p)
  } else {
    ns <- 0
  }
  
  # Overall recruitment rate for the remainder of the 3 year trial period
  full_sites <- which(setup_ts <= 3)
  time_recruiting <- pmin(3 - setup_ts, 3 - int_t)
  lambda <- sum(lambdas[full_sites]*(time_recruiting[full_sites]))
  
  # Expected number recruited at end of 3 years, given number at interim
  exp_n <- n_p + rpois(1, lambda) #lambda
  
  if(m_p > 0){
    df <- data.frame(y = ns,
                   c = 1:m_p)
  } else {
    df <- data.frame(y = 0, c = 1)
  }
  
  return(list(data = df, lambda = exp_n,
              stats = c(m_p, mean(ns/int_t), sd(ns/int_t))))
}
```

```{r, eval=FALSE}
m <- 20

# Upper threshold
#y <- c(6, 3, 9, 5, 7)

# Lower threshold
y <- c(3, 6)

int_data <- data.frame(y = y,
                       c = 1:length(y))

n_p <- sum(int_data$y)
m_p <- length(y)

bprior <- c(prior(normal(1.35, 0.5), class = "Intercept"),
            prior(student_t(10, 0, 0.11), class = "sd"))

#bprior <- c(prior(normal(1.35, 5), class = "Intercept"),
#            prior(student_t(10, 0, 1), class = "sd"))

fit  <- brm(y ~ 1 + (1 | c), data = int_data, family = poisson(),
            prior = bprior, silent = 2,
            iter = 3000, warmup = 500)

summary(fit, priors = TRUE)

s <- as_draws(fit)
beta_0 <- extract_variable(s, "b_Intercept")
sd_r <- extract_variable(s, "sd_c__Intercept")

# Posterior predicted random effects for the non-pilot sites
us <- sapply(1:(m-m_p), function(x) exp(rnorm(length(beta_0), beta_0, sd_r)))

# ...and for the sites included in the pilot
r <- ranef(fit, summary = F)
us <- cbind(us, exp(r$c[,1:m_p,1] + beta_0))

# Each column of u is a 6-monthly rate, each row a sample from the posterior.
# Want to weight sites by the time they recruit for. For the first m-m_p this
# will be random.
setup_rate_hps <- c(10 + m_p, 2 + 1)
setup_rates <- rgamma(nrow(us), setup_rate_hps[1], setup_rate_hps[2])
setup_times <- t(sapply(setup_rates, function(x) cumsum(rexp(m-m_p, x))))
setup_weights <- t(apply(5 - setup_times, 1, function(x) pmax(0, x)))
setup_weights <- cbind(setup_weights, matrix(rep(5, m_p*nrow(us)), ncol = m_p))

weighted_us <- us * setup_weights

# Overall expected recruited over the remaining 30 months
lambda <- rowSums(weighted_us)

# Simulate corresponding numbers recruited in main trial at 3 years
pred_n <- rpois(length(lambda), lambda) + n_p

#quantile(pred_n, c(0.05, 0.95))
#mean(pred_n > 320)

#saveRDS(pred_n, "pred_n_weak.rds")
#saveRDS(pred_n, "pred_n_inf.rds")
```

```{r}
df_w <- as.data.frame(readRDS("pred_n_weak.rds"))
names(df_w) <- "n"

df_i <- as.data.frame(readRDS("pred_n_inf.rds"))
names(df_i) <- "n"

p_w <- ggplot(df_w, aes(n)) + geom_histogram(fill = "grey", colour="black") +
  xlim(c(0,1500)) + xlab("Predicted final sample size") +
  theme_minimal() + 
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))

p_w <- p_w + geom_vline(xintercept = quantile(df_w$n, c(0.05, 0.95)), colour=cols[2], linetype=2, size = 2) 

p_i <- ggplot(df_i, aes(n)) + geom_histogram(fill = "grey", colour="black") +
  xlim(c(0,1500)) + xlab("Predicted final sample size") +
  theme_minimal() + 
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))

p_i <- p_i + geom_vline(xintercept = quantile(df_i$n, c(0.05, 0.95)), colour=cols[2], linetype=2, size = 2) 
```

## 

```{r, dev.args = list(bg = 'transparent')}
p_w
```

90% credible interval of [109, 1225], 0.41 probability that the trial will recruit to target.

## 

```{r, dev.args = list(bg = 'transparent')}
p_i
```

More informative priors give a 90% credible interval of [120, 425], with a 0.22 probability that the trial will recruit to target.

##

- If we would like more precision in our prediction, we could wait for a little longer so that we will have more data.
- To decide on how long to wait, we can calculate the average width of the 90% credible interval (the so-called Average Length Criterion).
- We could simulate lots of interim data, running our Bayesian analysis and recording the interval width each time. But this is slow.

# A detour

##

Consider this model for the number of success $y$ observed out of $n = 50$ trials:

$$
y \sim Bin(n, \theta) \\
\theta \sim Beta(a, b).
$$

Suppose we simulate $N$ pairs $(\theta_i, y_i)$ from the joint distribution $p(\theta, y)$, and then pick only the pairs where $y = 25$. Then then $\theta_i$'s from these pairs are a sample from the conditional distribution $p(\theta ~|~ y = 25)$:

## 

:::: {.columns}

::: {.column width="30%"}
$\theta$    | y
------------|------:
0.39        | 9
<span style="color:red;">0.31</span>        | <span style="color:red;">25</span> 
0.45        | 13
<span style="color:red;">0.64</span>         | <span style="color:red;">25</span> 
<span style="color:red;">0.58</span>         | <span style="color:red;">25</span> 
0.36        | 41
$\vdots$    | $\vdots$
:::

::: {.column width="10%"}
$\Rightarrow$
:::

::: {.column width="30%"}
$\theta$    | y
------------|------:
<span style="color:red;">0.31</span>        | <span style="color:red;">25</span> 
<span style="color:red;">0.64</span>         | <span style="color:red;">25</span> 
<span style="color:red;">0.58</span>         | <span style="color:red;">25</span> 
$\vdots$    | $\vdots$
:::

:::

##

```{r, dev.args = list(bg = 'transparent')}
p <- rbeta(10^5, 3, 3)
y <- rbinom(10^5, 50, p)

df <- data.frame(p = p[y==25])

ggplot(df, aes(p)) + geom_histogram(fill = "grey", colour="black") +
  theme_minimal() + geom_vline(xintercept = quantile(df$p, c(0.05, 0.95)), colour=cols[2], linetype=2, size=2) +
  xlab(expression(paste(theta, " | y"))) + 
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
```




## 

```{r}
library(extraDistr)

# Exact calculation
n_p <- 50
a <- 8; b <- 2
xs <- 0:n_p

# Get the CI lengths of every possible posterior
up <- sapply(xs, function(x) qbeta(0.95, a + x, b + n_p - x))
lo <- sapply(xs, function(x) qbeta(0.05, a + x, b + n_p - x))
l <- up - lo

# Plot the intervals
df_p <- data.frame(x = rep(xs, 2),
                   p = c(lo, up),
                   q = factor(rep(c(0.05, 0.95), each = length(xs))))

p <- ggplot(df_p, aes(x, p, group = q)) + geom_line(linetype=2) +
  xlab("Number of successes") + ylab("Probability") +
  theme_minimal() + 
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
```

```{r}
# Regression approximation
pr <- rbeta(10^5, a, b)
x <- rbinom(10^5, n_p, pr)
```

```{r, eval = FALSE}
t <- log(pr/(1-pr))

df <- data.frame(pr=pr, t=t, x=x)

qs <- c(0.05, 0.95)
fitq <- mqgam(t ~ s(x), data = df, qu = qs)
    
# For each point, get the estimated credible interval width
df$w_up <- qdo(fitq, qs[2], predict, newdata = df)
df$w_lo <- qdo(fitq, qs[1], predict, newdata = df)

df <- df[order(df$x),]

# Transform back to probability scale
df$w_up2 <- exp(df$w_up)/(1 + exp(df$w_up))
df$w_lo2 <- exp(df$w_lo)/(1 + exp(df$w_lo))

df_p2 <- data.frame(x = rep(df$x, 2),
                   p = c(df$w_lo2, df$w_up2),
                   q = factor(rep(c(0.05, 0.95), each = length(df$x))))

# Add some of the prior predictive samples
samps <- sample(1:nrow(df), 10^3)
df_p3 <- data.frame(x = df$x[samps],
                    p = df$pr[samps],
                    q=rep(1, 1000))

#saveRDS(df_p2, "df_p2.rds")
#saveRDS(df_p3, "df_p3.rds")
```

```{r, dev.args = list(bg = 'transparent')}
df_p2 <- readRDS("df_p2.rds")
df_p3 <- readRDS("df_p3.rds")

# Add to plot
p + geom_line(data = df_p2, colour = cols[1], size = 1) + geom_point(data = df_p3, alpha= 0.2)

# Get the empirical coverage of the approximate intervals
#mean(df$pr < df$w_up2 & df$pr > df$w_lo2)
```

Average length: 0.16. Coverage: 0.901.

# Application to GUSTO

##

To apply this approach in GUSTO, we fit a smooth additive model to estimate the quantiles of the distribution of final recruitment numbers `pred_n` against three statistics summarising the interim data: 

- the number of open sites `m_p`; 
- the average and standard deviations of the number of patients recruited per site, `mean_n` and `sd_n`.

```{r, echo = TRUE, eval = FALSE}
fitq <- mqgam(u ~  ti(m_p) + ti(mu_n) + ti(sd_n) + 
                  ti(m_p, mu_n) + ti(m_p, sd_n) + ti(mu_n, sd_n), 
              data = df, qu = c(0.05, 0.95))
```

## 

```{r, eval = FALSE}
m <- 20
int_ts <- seq(0.5, 2, 1/12)
rs <- NULL
for(int_t in int_ts){
  N <- 10^4
  
  bprior <- c(prior(normal(1.35, 0.5), class = "Intercept"),
            prior(student_t(10, 0, 0.11), class = "sd"))
  
  
  mu <- log(2*exp(rnorm(N, 1.35, 0.5)))
  sig <- rgamma(N, shape = 0.5*3, rate = 0.5*20)
  
  # Prior on setup rate
  setup_r <- rgamma(N, 20, 2)
  
  df <- data.frame()
  for(i in 1:N){
    # Simulate pilot data
    pilot_sim <- sim_rec_setup(m, int_t, mu[i], sig[i], setup_r[i])
    # Collect true recruitment rate plus three pilot data summary stats
    r <- c(log(pilot_sim$lambda), pilot_sim$stats[1], log(pilot_sim$stats[2] + 1), log(pilot_sim$stats[3] + 1))
    df <- rbind(df, r)
  }
  names(df) <- c("u", "m_p", "mu", "sd")
  
  qs <- c(0.05, 0.95)
  
  # Fit one overall GAM with m_p as a third dimension
  fitq <- mqgam(u ~  ti(mu) + ti(sd) + ti(m_p) + 
                  ti(mu, sd) + ti(m_p, mu) + ti(m_p, sd), data = df, qu = qs)
        
  w_up <- qdo(fitq, qs[2], predict, newdata = df)
  w_lo <- qdo(fitq, qs[1], predict, newdata = df)
  
  # Print any na's
  print(c(sum(is.na(w_up)), sum(is.na(w_lo))))
  
  # Transform to get the median width of the 90% Cr.I. for the number 
  # recruited at 36 months
  rs <- rbind(rs, c(int_t, 
                    mean(exp(w_up) - exp(w_lo), na.rm = T)))
  
  # empirical coverage
  print(mean(df$u < w_up & df$u > w_lo, na.rm = T))
}

#saveRDS(rs, "rs.rds")
```

```{r, dev.args = list(bg = 'transparent')}
rs <- readRDS("rs.rds")

df <- data.frame(t = 6:24,
                 w = rs[,2])

ggplot(df, aes(t, w)) + geom_point() +
  xlab("Interim analysis time (months)") +
  ylab("Average interval length") + 
  theme_minimal() + 
  theme(axis.text.x = element_text(size = 20),
        axis.text.y = element_text(size = 20),
        axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))
```

# Further work

## Application

- Combine this work (for internal pilots) with ICTMC poster (on external pilots);
- Extend from predicting number recruited to predicting feasibility in general;
- Consider if and how we can use this approach to pre-specify progression criteria;
- Work up a few illustrative examples.

## Methodology

- Design and run a simulation study comparing our approximation approach to the computationaly intensive nested Monte Carlo method;
- Give guidance of model diagnostics so we can have confidence in the approximation;
- Extend to Bayesian power calculations as an alternative basis for sample size determination.













