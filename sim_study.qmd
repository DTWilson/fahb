---
title: "fahb simulation study"
format: html
editor: visual
---

## Introduction

This document records the design and analysis of a simulation study comparing methods for making progression decisions based on internal pilot recruitment data.

## Variables and paramerters

Using the syntax of the code, we have the following design variables:

-   `m` - the number of sites to open over the full trial
-   `int_t` - the timing of the pilot interim analysis, expressed as a proportion of the expected time to recruit to target
-   `target_n` - the target sample size which will lead to recruitment stopping

We also have some model parameters:

-   `beta_0` and `sqrt(var_l)` - parameters for the log-normal distribution of the site recruitment rates
-   `setup_r` - the setup rate for sites

Becuase we have a Bayesian model, the parameters to vary in a simulation study are actually the hyper-parameters describing the priors for the three substantive parameters above. We have:

-   `beta_m` and `beta_s` - parameters of the normal prior on `beta_0`
-   `v_df` and `v_c` - parameters of the gamma prior on `sqrt(var_l)`
-   `setup_r_a` and `setup_r_b` - parameters of the gamma prior on `setup_r`

## Scenarios

We will define scenariuos by taking low or high values for:

-   The design parameters;
-   The expected prior values of the three substantive model parameters; and
-   An overall level of uncertainty in all three prior distributions.

For `m`, we will use 10 (low) or 30 (high).

For `int_t` we will use 0.2 (low) or 0.5 (high).

For `target_n` we will use 352 (low) or 788 (hgih). These correspond to total numbers of patients needed to detect an effect of 0.3 or 0.2 respectively, with a two-sided type I error rate of 0.05 and a power of 0.8.

For the expectation of `beta_0` we will use 6 (low) or 20 (high) patients per site per year.

For the expectation of `sqrt((var_l)` we will use 1 (low) or 3 (high).

For the expectation of `setup_r` we will use 5 (low) and 15 (high) sites per year.

For each of these parameters and each of these settings, we want a version with low prior uncertainty and one with high prior uncertainty. First, `beta_0`:

```{r}
# Low expectation, low variance
beta_m <- 1.79; beta_s <- sqrt(2*(log(6) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))

# Low expectation, high variance
beta_m <- 1.75; beta_s <- sqrt(2*(log(6) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))

# High expectation, low variance
beta_m <- 2.994; beta_s <- sqrt(2*(log(20) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))

# Low expectation, high variance
beta_m <- 2.954; beta_s <- sqrt(2*(log(20) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))
```

Now `sqrt(var_l)`:

```{r}
library(extraDistr)

# Low expectation (0.05), low variance
v_sh <- 100; v_r <- v_sh/0.05
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))

# Low expectation, high variance
v_sh <- 5; v_r <- v_sh/0.05
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))

# High expectation (0.3), low variance
v_sh <- 500; v_r <- v_sh/0.3
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))

# High expectation, high variance
v_sh <- 30; v_r <- v_sh/0.3
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))
```

And finally, `setup_r`:

```{r}
# Low expectation, low variance
setup_r_a <- 200; setup_r_b <- setup_r_a/5
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))

# Low expectation, high variance
setup_r_a <- 10; setup_r_b <- setup_r_a/5
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))

# High expectation, low variance
setup_r_a <- 1800; setup_r_b <- setup_r_a/15
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))

# High expectation, high variance
setup_r_a <- 90; setup_r_b <- setup_r_a/15
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))
```

Now we put all these variations into a .csv file to be read in during the batch. Note we will replicate each scenario 10 times, running 10\^4 simulations in each component.

```{r}
m <- c(10, 30)
int_t <- c(0.2, 0.5)
target_n <- c(352,  788)

rec_rate <- c(1,2)
rec_v <- c(1,2)
setup_r <- c(1,2)

# First, make a matrix of values for the low prior uncertainty case

low_unc <- expand.grid(m, int_t, target_n, rec_rate, rec_v, setup_r)
low_unc$beta_m <- c(1.79, 2.994)[low_unc[,4]]
low_unc$beta_s <- c(0.0593, 0.0588)[low_unc[,4]]
low_unc$v_sh <- c(100, 500)[low_unc[,5]]
low_unc$v_r <- c(2000, 1667)[low_unc[,5]]
low_unc$setup_r_a <- c(200, 1800)[low_unc[,6]]
low_unc$setup_r_b <- c(40, 120)[low_unc[,6]]

low_unc <- low_unc[,c(1:3, 7:12)]

# Similarly for high uncertainty

hi_unc <- expand.grid(m, int_t, target_n, rec_rate, rec_v, setup_r)
hi_unc$beta_m <- c(1.75, 2.954)[hi_unc[,4]]
hi_unc$beta_s <- c(0.2890, 0.2889)[hi_unc[,4]]
hi_unc$v_sh <- c(5, 30)[hi_unc[,5]]
hi_unc$v_r <- c(100, 100)[hi_unc[,5]]
hi_unc$setup_r_a <- c(10, 90)[hi_unc[,6]]
hi_unc$setup_r_b <- c(2, 6)[hi_unc[,6]]

hi_unc <- hi_unc[,c(1:3, 7:12)]

scenarios <- rbind(low_unc, hi_unc)
scenarios <- cbind(1:nrow(scenarios), scenarios)

# Replicate each scanrio 10 times to split across HPC
scenarios <- scenarios[rep(1:nrow(scenarios), each = 10),]
scenarios$part <- rep(1:10, 128)
scenarios <- scenarios[, c(1, 11, 2:10)]
names(scenarios)[1:5] <- c("id", "part", "m", "int_t", "target_n")

#write.csv(scenarios, file = paste0("R/scenarios.csv"), row.names = FALSE)
```

Each scenario will have a different expected time to recruit, which we will use to set our feasibility threshold - at 1.2 times this expectation.

```{r}
exp_rec_time <- function(sce){

  m <- sce[3]; target_n <- sce[5]
  beta_m <- sce[6]; beta_s <- sce[7]
  setup_r_a <- sce[10]; setup_r_b <- sce[11]
  
  # Expected rate of recruitment after all sites open
  final_rate <- exp(beta_m + beta_s^2/2)*m
  # Expected time until all sites open
  all_open <- m/(setup_r_a/setup_r_b)
  # Expected number recruited when all sites open
  n_0 <- final_rate*all_open/2
  if(n_0 >= target_n){
    exp_time <- sqrt(2*target_n*all_open/final_rate)
  } else {
    exp_time <- (target_n - (n_0 - final_rate*all_open))/final_rate
  }
  return(exp_time)
}

scenarios$exp_t <- apply(scenarios, 1, exp_rec_time)
scenarios$thr <- 1.2*scenarios$exp_t

#write.csv(scenarios, file = paste0("R/scenarios.csv"), row.names = FALSE)
```
## Analysis

For every scenario, find the FPR/FNR curves for both the PC and Bayes approaches to analysis. This takes a bit of time (especially the MCO for the PC approach), so do it once and save the results.

```{r}
full_res <- NULL
for(i in 1:128){
  print(i)
  res_comp <- PC_OCs_curve(i)
  res_comp <- cbind(res_comp, Bayes_OCs_curve(i)[,2])
  res_comp <- cbind(i, res_comp)
  full_res <- rbind(full_res, res_comp)
}

# Note - we are getting a v. small proportions (max 0.5%) of NAs in the Bayesian
# mean predictions in a handful of scenarios

for(i in 1:128){
  full_res[full_res$sce == i, 7] <- Bayes_OCs_curve(i)[,2]
}

#saveRDS(full_res, "full_res.rds")
```


```{r}
full_res <- as.data.frame(readRDS("full_res.rds"))
names(full_res) <- c("sce", "fpr", "fnr_PC", "red_m", "red_n", "red_r", "fnr_B")
full_res$fnr_dif <- full_res$fnr_PC - full_res$fnr_B

ggplot(full_res, aes(x = fnr_dif)) + stat_density(alpha = 0.4) +
  theme_minimal()

quantile(full_res$fnr_dif, c(0.9, 0.95, 0.99))
```

Look at redundancy in the optimal PC rules:

```{r}
colMeans(full_res[,4:6])
num_red <- full_res[,4] + full_res[,5] + full_res[,6]
mean(num_red == 0); mean(num_red == 1); mean(num_red == 2)
```
A specific scenario to focus on

```{r}
sub <- full_res[full_res$sce == 126,]
plot(sub[,c(2,3)])
points(sub[,c(2,7)], col="red")
```