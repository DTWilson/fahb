---
title: "fahb simulation study"
format: html
editor: visual
---

## Introduction

This document records the design and analysis of a simulation study comparing methods for making progression decisions based on internal pilot recruitment data.

## Variables and paramerters

Using the syntax of the code, we have the following design variables:

- `m` - the number of sites to open over the full trial
- `int_t` - the timing of the pilot interim analysis, expressed as a proportion of the expected time to recruit to target
- `target_n` - the target sample size which will lead to recruitment stopping

We also have some model parameters:

- `beta_0` and `sqrt(var_l)` - parameters for the log-normal distribution of the site recruitment rates
- `setup_r` - the setup rate for sites

Becuase we have a Bayesian model, the parameters to vary in a simulation study are actually the hyper-parameters describing the priors for the three substantive parameters above. We have:

- `beta_m` and `beta_s` - parameters of the normal prior on `beta_0`
- `v_df` and `v_c` - parameters of the gamma prior on `sqrt(var_l)`
- `setup_r_a` and `setup_r_b` - parameters of the gamma prior on `setup_r`

## Scenarios

We will define scenariuos by taking low or high values for:

- The design parameters;
- The expected prior values of the three substantive model parameters; and
- An overall level of uncertainty in all three prior distributions.

For `m`, we will use 10 (low) or 30 (high).

For `int_t` we will use 0.2 (low) or 0.5 (high).

For `target_n` we will use 352 (low) or 788 (hgih). These correspond to total numbers of patients needed to detect an effect of 0.3 or 0.2 respectively, with a two-sided type I error rate of 0.05 and a power of 0.8.

For the expectation of `beta_0` we will use 6 (low) or 20 (high) patients per site per year.

For the expectation of `sqrt((var_l)` we will use 1 (low) or 3 (high).

For the expectation of `setup_r` we will use 5 (low) and 15 (high) sites per year.

For each of these parameters and each of these settings, we want a version with low prior uncertainty and one with high prior uncertainty. First, `beta_0`:

```{r}
# Low expectation, low variance
beta_m <- 1.79; beta_s <- sqrt(2*(log(6) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))

# Low expectation, high variance
beta_m <- 1.75; beta_s <- sqrt(2*(log(6) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))

# High expectation, low variance
beta_m <- 2.994; beta_s <- sqrt(2*(log(20) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))

# Low expectation, high variance
beta_m <- 2.954; beta_s <- sqrt(2*(log(20) - beta_m))
beta_s
exp(beta_m + beta_s^2/2); sqrt(exp(beta_s^2 + 2)*sqrt(exp(beta_s^2) - 1))
```
Now `sqrt(var_l)`:

```{r}
library(extraDistr)

# Low expectation, low variance
v_sh <- 50; v_r <- v_sh/1
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))

# Low expectation, high variance
v_sh <- 10; v_r <- v_sh/1
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))

# High expectation, low variance
v_sh <- 450; v_r <- v_sh/3
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))

# High expectation, high variance
v_sh <- 90; v_r <- v_sh/3
v_r
v_sh/v_r; sqrt(v_sh/(v_r^2))
```

And finally, `setup_r`:

```{r}
# Low expectation, low variance
setup_r_a <- 200; setup_r_b <- setup_r_a/5
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))

# Low expectation, high variance
setup_r_a <- 10; setup_r_b <- setup_r_a/5
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))

# High expectation, low variance
setup_r_a <- 1800; setup_r_b <- setup_r_a/15
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))

# High expectation, high variance
setup_r_a <- 90; setup_r_b <- setup_r_a/15
setup_r_b
setup_r_a/setup_r_b; sqrt(setup_r_a/(setup_r_b^2))
```
Now we put all these variations into a .csv file to be read in during the batch. Note we will replicate each scenario 10 times, running 10^4 simulations in each component.

```{r}

```